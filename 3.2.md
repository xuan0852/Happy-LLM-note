## 3.2 Encoder-Decoder PLM
上一节提到了BERT，BERT 是一个基于 Transformer 的 Encoder-Only 模型，通过预训练任务 MLM 和 NSP 来学习文本的双向语义关系，从而在下游任务中取得了优异的性能。但是，BERT 也存在一些问题，例如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等问题。为了解决这些问题，研究者们提出了Encoder-Decoder模型。
### 3.2.1 T5
T5（Text-To-Text Transfer Transformer）,通过将所有 NLP 任务统一表示为文本到文本的转换问题，大大简化了模型设计和任务处理。T5 基于 Transformer 架构，包含编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。
#### 1.模型结构：Encoder-Decoder
![alt text](image-2.png)
从整体来看 T5 的模型结构包括 Tokenizer 部分和 Transformer 部分。Transformer 部分又分为 EncoderLayers 和 DecoderLayers 两部分，他们分别由一个个小的 Block组成，每个 Block 包含了多头注意力机制、前馈神经网络和 Norm 层。

T5 模型的 Encoder 和 Decoder 部分都是基于 Transformer 架构设计的，主要包括 Self-Attention 和前馈神经网络两种结构。Self-Attention 通过计算 Query、Key 和 Value 之间的相似度来捕捉输入序列中的全局依赖关系，前馈神经网络用于处理特征的非线性变换。
![alt text](image-3.png)
T5 模型的LayerNorm 采用了 RMSNorm，通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。RMSNorm只有一个可学参数，可以更好地适应不同的任务和数据集。
公式表示：
![alt text](image-4.png)
#### 2.预训练任务
训练所使用的数据集是一个大规模的文本数据集，包含了各种各样的文本数据，如维基百科、新闻、书籍等等。对数据经过细致的处理后，生成了用于训练的750GB 的数据集 C4，且已在 TensorflowData 中开源。
T5 的预训练任务，主要包括以下几个部分：
预训练任务：MLM
输入格式：文本
预训练数据集：C4
多任务训练
预训练到微调
#### 3.大一统思想
大一统思想，即所有的 NLP 任务都可以统一为文本到文本的任务。
![alt text](image-5.png)
对于不同的NLP任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。例如，任务前缀可以是“summarize: ”用于摘要任务，或“translate English to German: ”用于翻译任务。
大一统思想简化了任务处理流程，增强了模型的通用性和适应性。