# Chapter1 NLP基础概念
## 1.1什么是NLP
NLP是⼀种让计算机理解、解释和⽣成⼈类语⾔的技术。其核⼼任务是通过计算机程序来模拟⼈类对语⾔的认知和使⽤过程。NLP技术使得计算机能够执⾏各种复杂的语⾔处理任务，如中⽂分词、⼦词切分、词性标注、⽂本分类、实体识别、关系抽取、⽂本摘要、机器翻译、⾃动问答等。
## 1.2NLP发展历程
## 1.3NLP任务
### 1.3.1中文分词
由于中⽂语⾔的特点，词与词之间没有像英⽂那样的明显分隔（如空格），中⽂分词成为了中⽂⽂本处理的⾸要步骤，其⽬的是将连续的中⽂⽂本切分成有意义的词汇序列。
```
输入：今天天气真好
输出["今天"、"天气"、"真"、"好"]
```
### 1.3.2子词切分
⼦词切分是 NLP 领域中的⼀种常⻅的⽂本预处理技术，旨在将词汇进⼀步分解为更⼩的单位，即⼦词。当遇到罕⻅词或未⻅过的新词时，能够通过已知的⼦词单位来理解或⽣成这些词汇。
```
输出：unhappiness
使用子词切分："un","happi","ness"
```
### 1.3.3词性标注
词性标注是 NLP 领域中的⼀项基础任务，它的⽬标是为⽂本中的每个单词分配⼀个词性标签，如名词、动词、形容词等。这个过程通常基于预先定义的词性标签集。
假设有英文句子：She is playing the guitar in the park.
词性标注的结果为：
·She(Pronoun)
·is(Verb)
·playing(Verb)
·the(Determiner)
·guitar(Noun)
·in(Prep)
·the(Determiner)
·park(Noun)
·.(Punctuation)
### 1.3.4文本分类
将给定的⽂本⾃动分配到⼀个或多个预定义的类别中。⽂本分类的关键在于理解⽂本的含义和上下⽂，并基于此将⽂本映射到特定的类别。
### 1.3.5实体识别
旨在⾃动识别⽂本中具有特定意义的实体，并将它们分类为预定义的类别，如⼈名、地点、组织、⽇期、时间等。
假设有⼀个实体识别任务，⽬的是从⽂本中识别出⼈名、地名和组织名等实体。
```
输入：李雷和韩梅梅是北京市海淀区的居⺠，他们计划在2024年4⽉7⽇去上海旅⾏。
输出：：[("李雷", "⼈名"), ("韩梅梅", "⼈名"), ("北京市海淀区", "地名"), ("2024年4⽉7⽇", "⽇期"), 
("上海", "地名")]
```
### 1.3.6关系抽取
它的⽬标是从⽂本中识别实体之间的语义关系。这些关系可以是因果关系、拥有关系、亲属关系、地理位置关系等
### 1.3.7文本摘要
⽬的是⽣成⼀段简洁准确的摘要，来概括原⽂的主要内容。根据⽣成⽅式的不同，⽂本摘要可以分为两⼤类：抽取式摘要（Extractive Summarization）和⽣成式摘要（Abstractive Summarization）。
### 1.3.8机器翻译
指使⽤计算机程序将⼀种⾃然语⾔（源语⾔）⾃动翻译成另⼀种⾃然语⾔（⽬标语⾔）的过程。如基于神经⽹络的Seq2Seq模型、Transformer模型等
### 1.3.9自动问答
旨在使计算机能够理解⾃然语⾔提出的问题，并根据给定的数据源⾃动提供准确的答案。⾃动问答⼤致可分为三类：检索式问答（Retrieval-based QA）、知识库问答（Knowledge-based QA）和社区问答（Community-based QA）。
## 1.4文本表示的发展历程
⽂本表示的⽬的是将⼈类语⾔的⾃然形式转化为计算机可以处理的形式，也就是将⽂本数据数字化。
### 1.4.1词向量
向量空间模型（Vector Space Model, VSM）通过将⽂本（包括单词、句⼦、段落或整个⽂档）转换为⾼维空间中的向量来实现⽂本的数学化表示。在这个模型中，每个维度代表⼀个特征项（例如，字、词、词组或短语），⽽向量中的每个元素值代表该特征项在⽂本中的权重，这种权重通过特定的计算公式（如词频TF、逆⽂档频率TF-IDF等）来确定，反映了特征项在⽂本中的重要程度。
存在问题：数据稀疏性和维数灾难。
### 1.4.2语言模型
N-gram 模型是 NLP 领域中⼀种基于统计的语⾔模型，⼴泛应⽤于语⾳识别、⼿写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核⼼思想是基于⻢尔可夫假设，即⼀个词的出现概率仅依赖于它前⾯的N-1个词。
### 1.4.3 Word2Vec
旨在通过学习词与词之间的上下⽂关系来⽣成词的密集向量表示。Word2Vec的核⼼思想是利⽤词在⽂本中的上下⽂信息来捕捉词之间的语义关系，从⽽使得语义相似或相关的词在向量空间中距离较近。

Word2Vec模型主要有两种架构：
连续词袋模型CBOW(Continuous Bag of Words)是根据⽬标词上下⽂中的词对应的词向量, 计算并输出⽬标词的向量表示；
Skip-Gram模型与CBOW模型相反, 是利⽤⽬标词的向量表示计算上下⽂
中的词向量. 
CBOW适⽤于⼩型数据集, ⽽Skip-Gram在⼤型语料中表现更好。
### 1.4.4ELMo
ELMo（Embeddings from Language Models）
采⽤典型的两阶段过程: 
第1个阶段是利⽤语⾔模型进⾏预训练; 
第2个阶段是在做特定任务时, 从预训练⽹络中
提取对应单词的词向量作为新特征补充到下游任务中。
