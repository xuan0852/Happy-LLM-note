## 3.3 Decoder-Only PLM
### 3.3.1 GPT
GPT,即Generative Pre-Training Language Model。
#### 1.模型架构 Decoder Only
和BERT相比，GPT采用Decoder进行模型的堆叠。由于 Decoder-Only 结构也天生适用于文本生成任务，GPT 和 T5 的模型设计更契合于 NLG(Natural Language Generation) 任务和 Seq2Seq 任务。

对于一个自然语言文本的输入，先通过 tokenizer 进行分词并转化 input_ids。输入的 input_ids 首先通过 Embedding 层，再经过 Positional Embedding 进行位置编码。此处采用 Sinusoidal 位置编码，即通过三角函数进行绝对位置编码。通过 Embedding 层和 Positional Embedding 层编码成 hidden_states 之后进入Decoder。
#### 2.预训练任务 CLM
Decoder-Only 模型往往选择了最传统也最直接的预训练任务——因果语言模型，Casual Language Model，简称 CLM。
CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token。例如，CLM 的输入和输出可以是：
```
input: 今天天气
output: 今天天气很

input: 今天天气很
output：今天天气很好
```
#### 3.GPT系列模型的发展
略
### 3.3.2 LLaMA
LLaMA模型是由Meta开发的一系列大型预训练语言模型。
#### 1.模型架构——Decoder Only
与GPT系列模型一样，LLaMA模型也是基于Decoder-Only架构的预训练语言模型。LLaMA模型的整体结构与GPT系列模型类似，只是在模型规模和预训练数据集上有所不同。
#### 2.发展历程
略
### 3.3.3 GLM
GLM 系列模型是由智谱开发的主流中文 LLM 之一，包括 ChatGLM1、2、3及 GLM-4 系列模型，覆盖了指令理解、代码生成等多种应用场景，曾在多种中文评估集上达到 SOTA 性能(最优性能)。
#### 1.模型架构
整体结构上是 Decoder-Only 的结构，仅存在三点差异：
1.使用 Post Norm 而非 Pre Norm。Post Norm 是指在进行残差连接计算时，先完成残差计算，再进行 LayerNorm 计算；而类似于 GPT、LLaMA 等模型都使用了 Pre Norm，也就是先进行 LayerNorm 计算，再进行残差的计算。相对而言，Post Norm 由于在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好。(鲁棒性，又称稳健性，是衡量系统在异常条件或不确定性干扰下维持稳定表现的能力。)
2.使用单个线性层实现最终 token 的预测，而不是使用 MLP；这样的结构更加简单也更加鲁棒，即减少了最终输出的参数量，将更大的参数量放在了模型本身。
3.激活函数从 ReLU 换成了 GeLUs。
#### 2.预训练任务 GLM
GLM ，General Language Model，通用语言模型。
GLM 是一种结合了自编码思想和自回归思想的预训练方法。所谓自编码思想，其实也就是 MLM (Masked Language Modeling)的任务学习思路，在输入文本中随机删除连续的 tokens，要求模型学习被删除的 tokens；所谓自回归思想，其实就是传统的 CLM(Causal Language Modeling) 任务学习思路，也就是要求模型按顺序重建连续 tokens。
GLM 的核心思想是，对于一个输入序列，会类似于 MLM 一样进行随机的掩码，但每次遮蔽一连串 token；模型在学习时，既需要使用遮蔽部分的上下文预测遮蔽部分，在遮蔽部分内部又需要以 CLM 的方式完成被遮蔽的 tokens 的预测。例如，输入和输出可能是：
```
输入：I <MASK> because you <MASK>
输出：<MASK> - love you; <MASK> - are a wonderful person
```
通过将 MLM 与 CLM 思想相结合，既适配逐个 token 生成的生成类任务，也迫使模型从前后两个方向学习输入文本的隐含关系从而适配了理解类任务。
#### 3.发展
略