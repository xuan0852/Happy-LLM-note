## 3.1Encoder-only PLM
### 3.1.1 BERT
#### 1.思想沿承：
沿承Transformer架构，在此基础上将Encoder堆叠。
采用预训练+微调范式。
#### 2.模型架构：
采用Transformer的Encoder部分堆叠而成。不同点在于，BERT为了适应各种NLU任务，在模型顶层加了分类头prediction_heads，用于将多维度隐藏状态转换到分类维度。
此外，在注意力的计算过程中，BERT融入了相对位置编码，而Transformer采用绝对位置编码。
模型整体由Embedding,Encoder和prediction_heads组成。
#### 3.预训练任务 MLM+NSP
MLM：将文本序列中随机遮蔽部分token，将未被遮蔽的token输入模型，预测被遮盖的token。优点在于模型可以根据上文和下文一起理解语义，从而更好实现文本理解。
NSP：下一个句子预测。MLM在token级拟合予语义关系，而NSP在句级。
#### 4.下游任务微调
在海量无监督语料上进行预训练来获得通用的文本理解与生成能力，再在对应的下游任务上进行微调。微调指在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。
### 3.1.2 RoBERTa
以 BERT 为基础，在多个方向上进行优化。
#### 1.去掉NSP任务
实验结果表明，NSP 任务并不能提高模型性能，加入到预训练中并不能使下游任务微调时明显受益，甚至会带来负面效果。
#### 2.更大规模的预训练数据和预训练步长
RoBERTa 认为更大的 batch size 既可以提高优化速度，也可以提高任务结束性能。
#### 3.更大的bpe表
BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。
```
例如，对“Hello World”这句话，可能会切分为“Hel，lo，Wor，ld”四个子词对。
```
一般来说，BPE 编码的词典越大，编码效果越好。BERT 原始的 BPE 词表大小为 30K，RoBERTa 选择了 50K 大小的词表来优化模型的编码能力。
### 3.1.3 ALBERT
#### 1.将Embedding参数进行分解
ALBERT 对 Embedding 层的参数矩阵进行了分解，让 Embedding 层的输出维度和隐藏层维度解绑。
#### 2.跨层进行参数共享
通过对 BERT 的参数进行分析，ALBERT 发现各个 Encoder 层的参数出现高度一致的情况。因此，ALBERT 提出，可以让各个 Encoder 层共享模型参数，来减少模型的参数量。
在实现上，ALBERT 仅初始化了一个 Encoder 层，极大程度减小了模型参数量并且还提高了模型效果，但训练和推理时的速度相较 BERT 还会更慢。
#### 3.提出 SOP 预训练任务
类似于 RoBERTa，ALBERT 也认为 NSP 任务过于简单，ALBERT 选择改进 NSP，增加其难度，来优化模型的预训练。
传统的 NSP 任务中，正例是由两个连续句子组成的句对，而负例则是从任意两篇文档中抽取出的句对，模型可以较容易地判断正负例。而 SOP 任务提出的改进是，正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。模型不仅要拟合两个句子之间的关系，更要学习其顺序关系，这样就大大提升了预训练的难度。
实验证明:
使用 MLM + SOP 预训练的模型效果> 仅使用 MLM 预训练的模型 > 使用 MLM + NSP 预训练的模型。
总结：ALBERT 成功地以更小的参数实现了更强的性能，虽然由于其架构带来的训练、推理效率降低限制了模型的进一步发展，但打造更宽的模型这一思路仍然为众多更强大的模型提供了参考价值。